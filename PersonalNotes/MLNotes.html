<!DOCTYPE html>
<html lang="en">

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@4.0.0-alpha.1/es5/tex-mml-chtml.js"></script>
<script type="text/javascript">
MathJax = {
    tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
};
</script>

<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Machine Learning Notes</title>
<style>
/* Style for sidebar */
.sidebar {
    /* height: 100%; */
    height: calc(100vh - 40px);
    width: 200px;
    position: fixed;
    top: 0;
    left: 0;
    background-color: #333;
    padding-top: 20px;
    padding-bottom: 100px;
    overflow-y: auto;
}
  
/* Style for sidebar items */
.sidebar a {
    padding: 10px 15px;
    text-decoration: none;
    font-size: 15px;
    color: white;
    display: block;
}

/* Style for focus */
.sidebar a:focus {
    outline: none; /* Remove the default focus outline */
    color: black;
    position: relative;
    background-color: #fff;
    border-top-left-radius: 20px;
    border-bottom-left-radius: 20px;
    background-color: #fff;
}

/* Style for active */
.sidebar a:active {
    outline: none; /* Remove the default focus outline */
    color: black;
    position: relative;
    background-color: #fff;
    border-top-left-radius: 20px;
    border-bottom-left-radius: 20px;
    background-color: #fff;
}
/* Style for hovering */
.sidebar a:hover {
    color: black;
    background: #fff;
    outline: none;
    position: relative;
    background-color: #aaa;
    border-top-left-radius: 20px;
    border-bottom-left-radius: 20px;
}
/* Style for content */
.content {
    margin-left: 250px;
    padding: 20px;
}
/* Hide all content sections by default */
.section {
    display: none;
}
.sidebar a.active {
    color: black;
    background-color: #aaa;
    border-top-left-radius: 20px;
    border-bottom-left-radius: 20px;
}
/* Styles for the text body, bold and italic text */
body {
    background-color: black;
    color: white;
    font-family: Arial, sans-serif;
}
a {
    color: rgb(255, 1, 196);
}
b, strong {
    color: orange;
}
i, em {
    color: orchid;
}
/* style for header colors */
h1, h2, h3, h4, h5, h6 {
    color: #00bfff;
}
/* style for table */
table {
    border-collapse: collapse;
}
th, td, tr {
    border: 1px solid;
    text-align: center;
    padding: 5px;
    font-weight: 400
}
</style>
</head>

<body>





<div class="sidebar">
<a href="#section0" class="active">Content</a>
<a href="#section1">Data Preprocessing</a>
<a href="#section2">Supervised Learning</a>
<a href="#section3">Unsupervised Learning</a>
<a href="#section4">Neural Networks</a>
</div>









<div class="content">
<div id="section0" class="section">
<h1><a id="machine-learning-notes" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Machine Learning Notes</h1>
</div>
<div id="section1" class="section">
<h2><a id="data-preprocessing" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data Preprocessing</h2>
<h3><a id="data-scaling-and-standardizing" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data scaling And standardizing</h3>
\[x_i\leftarrow\dfrac{x_i-\mu}{\sigma}
\]
<h4><a id="pros-cons" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Pros &amp; Cons</h4>
<p>Dataset will be normalized, avoid unbalanced dataset</p>
<h4><a id="usage" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Usage</h4>
<p><code>sklearn.StandardScaler</code></p>
<h3><a id="imputation" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Imputation</h3>
<p>The process of replace missing values is known as data <em>imputation</em></p>
<h4><a id="examples" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Examples</h4>
<ul>
<li>Constant imputation: replace with contants</li>
<li>Linear interpolation/Regression Imputation: replace using a regression model</li>
<li>median/mean/mode/(sample statistic) imputation: replace with median/mean/mode/(sample statistic)</li>
<li>forward/backward fill: replace with previous/next value</li>
<li>KNN: replace using the mode the closest \(k\) neighbors</li>
</ul>
<h6><a id="remark" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Remark</h6>
<p>You should not use the labels or test data set to imputate training data set</p>
<h3><a id="pipelines" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Pipelines</h3>
<h5><a id="definition" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Definition</h5>
<p>A <em>pipeline</em> is a series of data processing components arranged sequentially, each component in the pipeline performs a specific task.</p>
<h6><a id="pros-cons" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Pros &amp; Cons</h6>
<p>This process streamlines the workflow, makes it easier to combine and expriment different algorithms and models.</p>
<h6><a id="example" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Example</h6>
<p>Learning cubic polynomial \(y=\beta_0+\beta_1x+\beta_2x^2+\beta_3x^3+\epsilon\)</p>
<pre><code class="language-plain_text">from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model.LinearRegression

pipe = Pipeline([
('poly', PolynomialFeatures(3, interaction_only=False, include_bias=False)),
('reg', LinearRegression(copy_X=True))
])
</code></pre>
<p><code>PolynomialFeatures</code> generate higher powers \(x^n\) from \(x\).</p>
</div>
<div id="section2" class="section">
<h2><a id="supervised-learning" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Supervised Learning</h2>
<p>Given Dataset \(D=\{(\mathbf x^{(i)},\mathbf y^{(i)})\}_{i=1}^N\), where \(\mathbf x^{(i)}\) are <em>feature vectors</em>, its entries are called <em>features</em>, and \(\mathbf y^{(i)}\) are <em>labels</em> or <em>predictions</em>. Assume \(\mathbf y=f(\mathbf x)+\boldsymbol\epsilon\) is the true relation, where \(f\) is a (typically continuous) function and \(\boldsymbol\epsilon\) is a random noise (typically \(\mathbb E(\boldsymbol\epsilon)=\mathbf 0\) and independent). <em>Supervised learning</em> is to &quot;learn&quot; a <em>model</em> \(\hat f\) of \(f\) and make predictions \(\hat{\mathbf y}=\hat f(\mathbf x)\).</p>
<h3><a id="bias-variance-trade-off" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bias-variance trade-off</h3>
<p>Suppose \(\mathbb E[\boldsymbol\epsilon]=\mathbf 0\), \(\hat f(\mathbf x)=\hat f(\mathbf x;D)\) with \(D\) sampled from joint probability distribution of \((\mathbf X,\boldsymbol\epsilon)\). Consider the expected total error at a fixed test input \(\mathbf x\) (so $\mathbb E=\mathbb E_{\mathbf X,\boldsymbol\epsilon|\mathbf X=\mathbf x}$)</p>
\[\begin{align*}
\mathbb E[\|\mathbf y-\hat{\mathbf y}\|^2] &amp;= \mathbb E[\|f+\boldsymbol\epsilon-\hat f\|^2]\\
&amp;=\mathbb E[\|f-\hat f\|^2]-2\mathbb E[(f-\hat f)\cdot\boldsymbol\epsilon]+\mathbb E[\|\boldsymbol\epsilon\|^2]\\
&amp;=\mathbb E[\|f-\hat f\|^2]-2\mathbb E[f-\hat f]\cdot\mathbb E[\boldsymbol\epsilon]+\mathbb E[\|\boldsymbol\epsilon\|^2]\\
&amp;=\mathbb E[\|f-\mathbb E\hat f+\mathbb E\hat f-\hat f\|^2]+\mathbb E[\|\boldsymbol\epsilon\|^2]\\
&amp;=\mathbb E[\|f-\mathbb E\hat f\|^2]+2\mathbb E[(f-\mathbb E\hat f)\cdot(\mathbb E\hat f-\hat f)]+\mathbb E[\|\mathbb E\hat f-\hat f\|^2]+\mathbb E[\|\boldsymbol\epsilon\|^2]\\
&amp;=\|f-\mathbb E\hat f\|^2+2(f-\mathbb E\hat f)\cdot\mathbb E[\mathbb E\hat f-\hat f]+\mathbb E[\|\mathbb E\hat f\|^2-2\mathbb E\hat f\cdot\hat f+\|\hat f\|^2]+\mathbb E[\|\boldsymbol\epsilon\|^2]\\
&amp;=\|f-\mathbb E\hat f\|^2+\mathbb E[\|\hat f\|^2]-\|\mathbb E\hat f\|^2+\mathbb E[\|\boldsymbol\epsilon\|^2]\\
&amp;=\|f-\mathbb E\hat f\|^2+\text{Var}[\hat f]+\mathbb E[\|\boldsymbol\epsilon\|^2]\\
\end{align*}
\]
<p>Here \(\sigma^2\) is referred to as the <em>irreducible error</em>, so we have the simplified version</p>
\[\text{total error = bias$^2$ + Variance + irreducible error}
\]
<p>When underfitting the model, the model is too simple so that the model bias is huge (e.g., using a linear equation approximate a quadratic). When overfitting the model, the model is too complex, so the model variance is great (e.g., use a high-degree polynomial to approximate a linear relation with small random noise). One has to make tradeoff between bias and variance so that both aren't significant.</p>
<h3><a id="objective-loss-function" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Objective &amp; Loss function</h3>
<p>To improve the model, we need loss functions</p>
<ul>
<li><em>Mean squared error (MSE)</em>: $\displaystyle\frac{1}{N}\sum_{i=1}^N|\mathbf y^{(i)}-\hat{\mathbf y}^{(i)}|^2$. Used in regression</li>
<li><em>Mean Absolute Error (MAE)</em>: $\displaystyle\frac{1}{N}\sum_{i=1}^N|\mathbf y^{(i)}-\hat{\mathbf y}^{(i)}|$.</li>
<li><em>Root Mean Square Error (RMSE)</em>: $\displaystyle\sqrt{\frac{1}{N}\sum_{i=1}^N|\mathbf y^{(i)}-\hat{\mathbf y}^{(i)}|^2}$.</li>
<li><em>Logistic Loss or Cross-Entropy Loss</em>: \(\displaystyle-\sum_i[y_i\log(\hat p_i)+(1-y_i)\log(1-\hat p_i)]\). Used in binary classification. Or \(\displaystyle-\sum_{c=1}^C\sum_{i}y_{i,c}\log(\hat p_{i,c})\). Used in multiclass classification</li>
</ul>
<p>But to prevent overfitting, we also include a regularization term \(\Omega(\theta)\). The objective function is then the sum of \(L(\theta)\) and \(\Omega(\theta)\)</p>
<h3><a id="k-fold-cross-validation-grid-search" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>\(k\)-fold cross validation &amp; grid search</h3>
<ol>
<li><em>\(k\)-fold cross validation</em> divide the dataset into \(k\) subsets. Train the model on \(k-1\) subsets independently \(k\) times by single out each as the validation set. And eventually take the average of the parameters.</li>
<li><em>Grid search</em> provide an array of values for each parameter and test model with every value and choose the best one.</li>
</ol>
<h4><a id="usage" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Usage</h4>
<pre><code class="language-plain_text">from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error

GridSearchCV(
cv = KFold(n_splits=5, random_state=30293, shuffle=True),
estimator = KNeighborsRegressor(),
param_grid = {
    'n_neighbors': range(1, 50),
    'weights': ['uniform', 'distance']
},
scoring = 'neg_mean_squared_error'
)
</code></pre>
<h6><a id="remark" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Remark</h6>
<p>When should you not use cross-validation, and use simple validation instead?</p>
<ol>
<li>Dataset size is too small. This can lead to deficiencies in both model fitting and estimation.</li>
<li>Model training time is too long. It might not worth the time.</li>
</ol>
<h3><a id="gradient-descent" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Gradient descent</h3>
<p>The method of <em>gradient descent</em> is to decrease the loss function \(\ell\) by \(\beta\leftarrow\beta-\alpha\nabla(\beta)\).<br />
Some common adjustments are</p>
<ol>
<li><em>Mini-batch gradient descent</em>: instead of use the entire dataset, cycling through mini batches to generate gradients.</li>
<li><em>Stochastic gradient descent</em>: Randomly generates learning rates \(\alpha\) each time.
<ul>
<li>Pros: Avoid of being stuck in a local minimum.</li>
</ul>
</li>
</ol>
<h4><a id="comparisons-of-common-gradient-descent-methods" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Comparisons of common gradient descent methods</h4>
<ul>
<li><em>Stochastic gradient descent(SGD)</em> is to update the parameter according to individual gradient<br />
Gradient descent is
\[\theta_{t+1}=\theta_t-\lambda\cdot\nabla L(\theta_t)
\]
And SGD is when \(L(\theta)=\dfrac{1}{N}\sum_iL_i(\theta)\)</li>
<li><em>Momentum</em> \(v\) is defined by
\[\begin{cases}
v_{t+1}=\beta\cdot v_t+(1-\beta)\cdot\nabla L(\theta_t)\\
\theta_{t+1}=\theta_t-\lambda\cdot v_{t+1}
\end{cases}
\]
This includes the &quot;inertia&quot; from the previous momentums and gradients, it helps accelerate convergence in the direction of persistent gradient, and reduce oscillations.</li>
<li><em>Adaptive gradient(Adagrad)</em> is mathematically described by
\[\begin{cases}
G_{t+1}=G_t+|\nabla L(\theta_t)|^2\\
\theta_{t+1}=\theta_t-\dfrac{\lambda}{\sqrt{G_{t+1}+\epsilon}}\cdot \nabla L(\theta_{t+1})
\end{cases}
\]
\(G_t\) is the accumulated squared gradients (like the second momentum). This method ensures that the learning rate doesn't get too small when having really large gradients. This method is good with sparse data but might overly reduce learning rate when encountering some frequently occuring features with large gradients.</li>
<li><em>Root mean squared propagation(RMSprop)</em> is slightly changing Adagrad
\[\begin{cases}
G_{t+1}=\beta\cdot G_t+(1-\beta)|\nabla L(\theta_t)|^2\\
\theta_{t+1}=\theta_t-\dfrac{\lambda}{\sqrt{G_{t+1}+\epsilon}}\cdot \nabla L(\theta_{t+1})
\end{cases}
\]
This method helps mitigate the problem of diminishing learning rate</li>
<li><em>Adaptive moment estimate(Adam)</em> combines momentum and RMSProp
\[\begin{cases}
m_{t+1}=\beta_1\cdot m_t+(1-\beta_1)\cdot\nabla L(\theta_t)\\
v_{t+1}=\beta_2v_t+|\nabla L(\theta_t)|^2\\
\hat m_{t+1} = \dfrac{m_t}{1-\beta_1^{t+1}}\\
\hat v_{t+1} = \dfrac{v_t}{1-\beta_2^{t+1}}\\
\theta_{t+1}=\theta_t-\dfrac{\lambda}{\sqrt{\hat v_{t+1}+\epsilon}}\cdot \hat m_{t+1}
\end{cases}
\]
The normalization prevents bias from early initialization (for example, \(m_0=v_0=0\), dividing \(1-\beta_1,1-\beta_2\) could make them less biased, as time progress, \(\beta^t\) has exponential decay and goes to 0 and has no effect in normalizing).</li>
</ul>
<h3><a id="regularization" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Regularization</h3>
<p>Regularization is adding penalty terms to reduce the loss function. It controls the magnitude of the feature vector \(\beta\)</p>
<ol>
<li><em>Ridge regularization</em> is to add $\lambda|\beta|_2^2$</li>
<li><em>Lasso regularization</em> is to add $\lambda|\beta|_1$</li>
</ol>
<h4><a id="pros-cons" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Pros &amp; Cons</h4>
<ol>
<li>Lasso works better for feature selection, so it is better if there are a large amount of features. But it might only randomly choose some of highly correlated features (colinearity).</li>
<li>Ridge is better if it depends on almost all the features, because it handles colinearity better. However it is computationally costly with a large number of predictors</li>
</ol>
<h4><a id="elastic-net" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Elastic net</h4>
<p>Sometimes it might be better to simply use the <em>elastic net</em> regurlarization which add $\lambda_1|\beta|_1+\lambda_2|\beta|_2^2$</p>
<h3><a id="confusion-matrix" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Confusion matrix</h3>
<p>The <em>confusion matrix</em> is the \(2\times2\) contingency table, where the rows are the predicted values, and columns are the actual values.</p>
<table>
<thead>
<tr>
<th></th>
<th>Positive</th>
<th>Negative</th>
</tr>
</thead>
<tbody>
<tr>
<td>Positive</td>
<td>TP</td>
<td>FP</td>
</tr>
<tr>
<td>Negative</td>
<td>FN</td>
<td>TN</td>
</tr>
</tbody>
</table>
<p>We define</p>
<ul>
<li><em>Accuracy</em> = \(\dfrac{TP+TN}{TP+FP+FN+TN}\)<br />
Accuracy is used if the dataset is balanced and equally distributed, e.g. spam detection</li>
<li><em>Precision</em> = \(\dfrac{TP}{TP+FP}\)<br />
Precision is used if the cost for false positive is high, e.g. Fraud detection</li>
<li><em>Recall(Sensitivity)</em> = \(\dfrac{TP}{TP+FN}\)<br />
Recall is used if the cost for false negative is high, e.g. disease detection</li>
<li><em>Specificity</em> = \(\dfrac{TN}{TN+FP}\)</li>
<li><em>F1 score</em> = harmonic mean of Precision and Recall, i.e.</li>
</ul>
\[\text{F1 score} = \dfrac{2}{\dfrac{1}{\text{Precision}}+\dfrac{1}{\text{Recall}}} = 2\dfrac{\text{Precision}\times\text{Recall}}{\text{Precision}+\text{Recall}}
\]
<p>F1 score is the single metric of both the precision and recall which balances the Precision-Recall tradeoff by taking both into account, especially if there is an uneven class distribution, e.g. search engine ranking for relevance.</p>
<h3><a id="roc" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>ROC</h3>
<p>In binary classification, \(\hat Y\) is usually a continuous random variable. <em>Receivers operating characteristic curve (ROC)</em> is the parametrized curve \((\text{fpr}(t),\text{tpr}(t))\), \(t\in\mathbb R\) where</p>
<ul>
<li>
<p>$\displaystyle\text{tpr(t)}=\frac{TP}{TP+FN}=\mathbb P(\hat Y\geq t|Y=1)$ is true positive rate (recall)</p>
</li>
<li>
<p>$\displaystyle\text{fpr(t)}=\frac{FP}{FP+TN}=\mathbb P(\hat Y\geq t|Y=0)$ is false positive rate (1-specificity)</p>
</li>
<li>
<p>\(t\) is cut-off</p>
</li>
</ul>
<p>It is not hard to conclude</p>
<ul>
<li>A total random model corresponds to the diagonal line, where \(\hat Y\) is independent of \(Y\) and thus \(\text{tpr}(t)=\text{fpr}(t)=\mathbb P(\hat Y\geq t)\)</li>
<li>The perfect model corresponds to two segments \((0,0)\to(0,1)\) and \((0,1)\to(1,1)\), where \(\mathbb P(\hat Y\geq t_0)=\mathbb P(Y=1)\) for some \(t_0\), and \(\text{tpr}(t)=1,\text{fpr}(t)=0\)</li>
<li>\(\text{tpr}(-\infty)=\text{fpr}(-\infty)=1\), \(\text{tpr}(\infty)=\text{fpr}(\infty)=0\), \(\text{tpr},\text{fpr}\) are non-increasing</li>
</ul>
<p>The <em>Area under ROC (AUROC/AUC)</em> measures a comprehensive classifier's performance, if it is \(\frac{1}{2}\), and it is like random, if it is 1, then it is outstanding discrimination. AUC is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one. Suppose $Z_1\sim\hat Y|Y=1$ has cdf \(1-\text{tpr}\) and $Z_0\sim\hat Y|Y=0$ has cdf \(1-\text{fpr}\) are independent</p>
\[\begin{align*}
\text{AUC}&amp;=\int_0^1ydx=\int_{+\infty}^{-\infty}\text{tpr}(t)d\text{fpr}(t)\\
&amp;=\int_{+\infty}^{-\infty}\text{tpr}(t)\text{fpr}'(t)dt\\
&amp;=\int_{+\infty}^{-\infty}\left(-\int_t^\infty\text{tpr}'(s)ds\right)\text{fpr}'(t)dt\\
&amp;=\int_{-\infty}^\infty\int_{-\infty}^\infty\text{tpr}'(s)\text{fpr}'(t)\mathbf 1_{s\geq t}(s,t)dsdt\\
&amp;=\mathbb P(Z_1\geq Z_0)
\end{align*}
\]
<p>Suppose ${Z_0^{(i)}}<em>{i=1}^{n_0}\sim\hat Y|Y=0$, ${Z_1^{(j)}}</em>{j=1}^{n_1}\sim\hat Y|Y=1$ are (independent) samples, then an unbiased estimator of AUC is \(\dfrac{U}{n_0n_1}\), where</p>
\[U=\sum_{i=1}^{n_0}\sum_{j=1}^{n_1}\mathbf 1_{Z_1^{(j)}\geq Z_0^{(i)}}=n_0n_1+\frac{n_0(n_0+1)}{2}-R_0
\]
<p>Note that this is precisely the <em>Wilcoxon-Mann-Whitney (WMW)</em> \(U\)-statistic. Where \(R_0\) is the sum of ranks \(Z_0^{(i)}\) in all of \(Z\)'s</p>
<ul>
<li>Proof: Suppose the ranks of \(Z_0^{(i)}\) are \(r_1&lt;\cdots&lt;r_{n_0}\), then \(R_0=r_1+\cdots+ r_{n_0}\) and
\[\begin{align*}
U&amp;=(n_0+n_1-r_{n_0})+(n_0+n_1-1-r_{n_0-1})+\cdots+(n_0+n_1-(n_0-1)-r_1)\\
&amp;=n_0(n_0+n_1)-R_1-\frac{n_0(n_0-1)}{2}\\
&amp;=n_0n_1+\frac{n_0(n_0+1)}{2}-R_0
\end{align*}
\]
</li>
</ul>
<h3><a id="r-squared" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>R Squared</h3>
<p><em>Total sum of squares</em> \(SS_{tot}=\sum_i(y_i-\bar y)^2\)</p>
<p><em>Residual sum of squares</em> \(SS_{res}=\sum_i(y_i-\hat y)^2\)</p>
<p><em>Coefficient of determination (\(R^2\))</em> is defined to be \(1-\dfrac{SS_{res}}{SS_{tot}}\)</p>
<p>If \(R^2=0\), it means the model have worst predictions since it is a constant average prediction, if \(R^2=1\), then the model is accurate.</p>
<p>GAIN/LIFT charts</p>
<h3><a id="k-nearest-neighbors" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>\(k\)-nearest neighbors</h3>
<p>The <em>\(k\)-nearest neighbors</em> algorithm assigns the most likely label from the nearest neighbors.</p>
<h3><a id="linear-and-logistic-regression" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Linear and logistic regression</h3>
<p><em>Logistic regression</em> used in binary classification by \(p(x)=\dfrac{1}{1+e^{-\beta x}}\).</p>
<h4><a id="interaction-terms-in-linear-regression" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Interaction terms in linear regression</h4>
<p>When you have categorical vairables, you should add interaction terms since it might has a impact on other variables.</p>
<h4><a id="residual-plots" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Residual plots</h4>
<ul>
<li>Residual vs features. It helps find missing signals and identify missing interaction terms.</li>
<li>Residual vs predicted.</li>
</ul>
<h4><a id="feature-selection" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Feature selection</h4>
<p>The <em>best subsets selection</em> tries every possible subset of features and then choose the best one. This is very computational costly. Instead we could do</p>
<ul>
<li><em>Forwards selection</em>: Start with baseline model (no features selected), and each step, try all the remaining features with the current model, choose the best performing one (minimal MSE), and discard the others, and iterate, if non is better than current, then stop and use current model.</li>
<li><em>Backwards selection</em>: Start with a model that includes all features, then lose features one at a time. If losing any is worse, then stop and use current model.</li>
</ul>
<p>We could also try simply lasso regularization.</p>
<h4><a id="regressino-version-of-classification-algorithms" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Regressino version of classification algorithms</h4>
<ol>
<li><em>\(k\) nearest neighbors regression</em> takse the average of the \(k\) nearest values.</li>
<li><em>Tree regression</em> use MSE as loss function</li>
<li><em>Supported vector regression</em></li>
</ol>
<h3><a id="supported-vector-machine" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Supported vector machine</h3>
<p>In binary classification, given a dataset \(\{(\mathbf x_i,y_i)\}_{i=1}^N\), where \(y_i=\pm1\) is the label. Naively, <em>supported vector machine</em> is used to find a border that maximize the margin between two classes.</p>
<h4><a id="hard-margin" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Hard margin</h4>
<p>If the data is linear separable, we wish to find a hyperplane \(\mathbf w\cdot\mathbf x-b=\mathbf0\) that separate these two classes with maximal margin. Equivalently, it is to solve the following problem: Find \(\mathbf w\) and \(b\) that minimize $|\mathbf w|_2^2$ and subject to</p>
\[y_i(\mathbf w\cdot\mathbf x_i-b)\geq 1
\]
<p>The geometric interpretation depends on the fact:</p>
\[\text{The distance between the origin and the plane $\mathbf w\cdot\mathbf x-b=0$ is $\frac{|b|}{\|\mathbf w\|_2}$}
\]
<p>We want to choose \(\mathbf w,b\) such that \(\mathbf w\cdot\mathbf x-b=1\), \(\mathbf w\cdot\mathbf x-b=-1\) barely touches two classes. So the margin between each class and the border would be $\frac{1}{|\mathbf w|_2}$. Note that this max-margin hyperplane is completely determined by those \(\mathbf x_i\) that lie nearset to it, they are called <em>support vectors</em>.</p>
<h4><a id="hinge-loss" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Hinge loss</h4>
<p>The <em>hinge loss</em> is a function like \(\ell(y)=\max(0,1-t\cdot y)\).</p>
<h4><a id="soft-margin" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Soft margin</h4>
<p>If the dataset is not linearly separable, we introduce the <em>hinge function</em> \(\max(0,1-y_i(\mathbf w\cdot\mathbf x_i-b))\), this penalize data on the wrong side of the margin. We can define a loss function</p>
\[\lambda\|\mathbf w\|_2^2+\frac{1}{N}\sum_{i=1}^N\max(0,1-y_i(\mathbf w\cdot\mathbf x_i-b))
\]
<p>If \(\lambda\) is small, then it is basically hard-margin SVM. A soft-margin optimization problem could be to minimize $\lambda|\mathbf w|<em>2^2+\frac{1}{N}\sum</em>{i=1}^N\zeta_i$ subject to</p>
\[y_i(\mathbf w\cdot\mathbf x_i-b)\geq 1-\zeta_i,\quad \zeta_i\geq0
\]
<h4><a id="nonlinear-kernels" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Nonlinear kernels</h4>
<p>Sometimes it is very hard to separate data, we consider transformations \(\varphi\) that takes \(\mathbf x_i\) into higher dimensional spaces (even infinite dimensions!). And if we make sufficiently good choices, we don't need to care what \(\varphi\) really does and we simply need to know what \(\kappa(\mathbf x_i,\mathbf x_j)=\varphi(\mathbf x_i)\cdot\varphi(\mathbf x_j)\) is, \(\kappa\) is called <em>kernel</em>. Common examples are</p>
<ol>
<li>Linear: \(\kappa(\mathbf x_i,\mathbf x_j)=\mathbf x_i\cdot\mathbf x_j\).</li>
<li>Polynomlial: \(\kappa(\mathbf x_i,\mathbf x_j)=(\mathbf x_i\cdot\mathbf x_j+r)^d\).<br />
Note that for example if we choose \(\varphi(x_1,x_2)=(x_1^2,\sqrt2x_1x_2,x_2^2)\), then
\[\varphi(\mathbf x)\cdot\varphi(\mathbf y)=x_1^2x_2^+2x_1x_2y_2y_2+y_1^2y_2^2=(x_1y_1+x_2y_2)^2=(\mathbf x\cdot\mathbf y)^2
\]
</li>
<li>Gaussian Radial Kernel: $\kappa(\mathbf x_i,\mathbf x_j)=\exp(-\gamma|\mathbf x_i-\mathbf x_j|_2^2)$.</li>
<li>Sigmoid: \(\kappa(\mathbf x_i,\mathbf x_j)=\tanh(\gamma\mathbf x_i\cdot\mathbf x_j+r)\).</li>
</ol>
<p>We can solve the dual optimization problem.</p>
<h3><a id="bayes-based-classifiers" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bayes' based classifiers</h3>
<h4><a id="linear-discriminant-analysis-lda" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Linear discriminant analysis (LDA)</h4>
<p>Assume $X|y=c\sim\mathcal N(\mu_c,\sigma^2)$, in the case where \(X\) has one feature, we have</p>
\[f_c(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu_c)^2}{2\sigma^2}\right)
\]
<p>Then Bayes' rule tells us</p>
\[P(y=c|X)=\frac{\pi_c\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu_c)^2}{2\sigma^2}\right)}{\sum_{l=1}^C\pi_l\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu_l)^2}{2\sigma^2}\right)}
\]
<p>Here \(\pi_c\) denotes \(P(y=c)\). So we could estimate</p>
\[\hat\mu_c=\frac{1}{N_c}\sum_{y_i=c}X_i
\]
\[\hat\sigma^2=\frac{1}{N-C}\sum_{c=1}^C\sum_{y_i=c}(X_i-\hat\mu_c)^2
\]
<p>We make predictions by choosing \(c\) rendering maximum likelihood $P(y=c|X)$, this is equivalent to choose largest <em>discriminant function</em></p>
\[\delta_c(X)=X\frac{\mu_c}{\sigma^2}-\frac{\mu_c^2}{2\sigma^2}+\log(\pi_c)
\]
<p>Here we should use \(\hat\mu_c,\hat\sigma\). In the case where \(X\) has \(m\) features, we have $X|y=c\sim\mathcal N(\mu_c,\Sigma)$, and</p>
\[f_c(\mathbf x)=\frac{1}{(2\pi)^{m/2}|\Sigma|^{1/2}}\exp\left(-\frac{1}{2}(\mathbf x-\mu_c)^T\Sigma^{-1}(\mathbf x-\mu_c)\right)
\]
<p>And the discriminant function will be</p>
\[\delta_c(X)X^T\Sigma^{-1}\mu_c-\frac{1}{2}\mu_c^T\Sigma^{-1}\mu_c+\log(\pi_c)
\]
<h4><a id="quadratic-discriminant-analysis-qda" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Quadratic discriminant analysis (QDA)</h4>
<p>Assume $X|y=c\sim\mathcal N(\mu_c,\Sigma_c)$, we get discriminant</p>
\[\begin{align*}
\delta_c(X)&amp; = -\frac{1}{2} \left( X - \mu_c \right)^T \Sigma_c^{-1}  \left(X- \mu_c  \right) - \frac{1}{2}\log\left(|\Sigma_c| \right) + \log(\pi_c)\\
&amp;= -\frac{1}{2} X^{T} \sigma^{-1}_c X + X^{T} \sigma^{-1}_c \mu_c - \frac{1}{2} \mu_c^T \sigma_c^{-1} \mu_c - \frac{1}{2}\log\left(|\Sigma_c| \right) + \log(\pi_c)
\end{align*}
\]
<h4><a id="naive-bayes-classifier" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Naive Bayes classifier</h4>
<p>Assume for each given class \(c\), each of the \(m\) features are independent, we then have</p>
\[f_c(X)=f^{(1)}_c(X_1)\cdots f^{(m)}_c(X_m)
\]
<p>Then by Bayes rule</p>
\[P(y=c|X)=\frac{\pi_cf^{(1)}_c(X_1)\cdots f^{(m)}_c(X_m)}{\sum_{l=1}^C\pi_lf^{(1)}_l(X_1)\cdots f^{(m)}_l(X_m)}
\]
<p>To estimate \(f^{i}_c\) we assume some kind of distribution and hten estimate the parameters</p>
<ul>
<li>If \(X_i\) are quantitative, we assume it is a normal distribution</li>
<li>If \(X_i\) are categorical, we assume it is a Bernouli distribution</li>
</ul>
<h4><a id="pros-cons" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Pros &amp; Cons</h4>
<ol>
<li>LDA works better for smaller datasets and QDA works for large datasets</li>
<li>LDA works better if the data can be mostly separated by linear decision boundaries. QDA works better if the decision boundaries are not linear.</li>
<li>If we have really small amount of data, we can use naive Bayes model. This in general a decent classifier.</li>
</ol>
<h3><a id="decision-trees" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Decision trees</h3>
<h4><a id="pros-cons" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Pros &amp; Cons</h4>
<ul>
<li>Pros
<ul>
<li>Very fast and very needs little data preprocessing</li>
</ul>
</li>
<li>Cons
<ul>
<li>This algorithm is greedy, so might not create an optimal tree</li>
<li>Decision trees have orthogonal boundaries, which might not be ideal</li>
<li>Decision trees are sensitive to training data</li>
</ul>
</li>
</ul>
<h4><a id="gini-impurity" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Gini impurity</h4>
<p><em>Gini impurity</em> \(I_G\) is defined by</p>
\[I_G(p)=\sum_ip_i(1-p_i)=\sum_i(p_i-p_i^2)=1-\sum_ip_i^2
\]
<p>\(I_G(p)\) is between 0 and 1, if \(I_G(p)=0\), then it is of a single class, if it is \(1-\dfrac{1}{N}\), it is evenly distributed.</p>
<h4><a id="cross-entropy" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Cross entropy</h4>
<p>The <em>information content (surprisal)</em> of an event \(A\) is quantified as \(\log\left(\dfrac{1}{P(A)}\right)=-\log P(A)\). The expected surprisal of \(A\) is \(-P(A)\log P(A)\). The <em>Entropy</em> under \(P\) is the sum of expected surprisal</p>
\[H(P)=-\mathbb E_P[\log P]=-\sum_ip_i\log(p_i)
\]
<p>The <em>Cross-entropy</em> of \(Q\) under \(P\) is</p>
\[H(P,Q)=-\mathbb E_P[\log Q]=-\sum_ip_i\log(q_i)
\]
<p>which measures the discrepancy using \(Q\) as predictions given the actual distribution is \(P\).</p>
<p>The <em>(KL convergence)</em> of \(P\) from \(Q\) is</p>
\[D_{KL}(P||Q)=\sum_ip_i\log\left(\dfrac{p_i}{q_i}\right)=H(P,Q)-H(P)
\]
<p>This is always nonnegative (<em>Gibb's inequality</em>) since</p>
\[\begin{align*}
-D_{KL}(P||Q)&amp;=\sum_ip_i\ln\left(\dfrac{q_i}{p_i}\right)\\
&amp;\leq\sum_ip_i\left(\dfrac{q_i}{p_i}-1\right)\\
&amp;=\sum_iq_i-\sum_ip_i\\
&amp;=0
\end{align*}
\]
<p>So the minimum of the cross entropy \(H(P)\) is attained \(\iff P=Q\)</p>
<p>Note that cross entropy and relative entropy measures the same because the entropy of \(P\) is fixed.</p>
<h4><a id="cart-algorithm" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>CART algorithm</h4>
<p>The <em>CART</em> (Classification and Regression Trees) algorithm is a decision tree-based algorithm that can be used for both classification and regression problems in machine learning. It works by recursively partitioning the training data into smaller subsets using binary splits.</p>
<h4><a id="random-forest" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Random Forest</h4>
<p>The <em>random forest</em> model is made by building many different decision trees. These trees are made &quot;different&quot; through a variety of random perturbations. Finally take the average of all trees.</p>
<p><a href="https://xgboost.readthedocs.io/en/stable/tutorials/model.html">XGBoost Tutorials</a></p>
<h4><a id="boosting" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Boosting</h4>
<p>A statistical learning algorithm is said to be a</p>
<ul>
<li><em>weak learner</em> if it does slightly better thant random guessing.</li>
<li><em>strong learner</em> if can be made arbitrarily close the true relationship</li>
</ul>
<p>Thanks to PAC (Probably approximately correct) learnability, one can show that there exists <em>boosting</em> algorithms that can turn weak learners into strong learners.</p>
<p>For example a decision tree with a single layer (decision stump) is a weak learner, whereas a decision tree is a strong leaner.</p>
<h4><a id="adaptive-boosting" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Adaptive boosting</h4>
<p><em>Adaptive boosting</em> is building stronger learners iteratively by learning the weakness of the previous weak leaner. Suppose we have iteratively built up the first \(j\) weak learners, we now construct the \(j+1\)-th weak learner. Suppose the prediction of \(y_i\) by the \(j\)-th weak learner is \(\hat y^{(j)}_i\), and assume the current weight assigned to \(y_i\) is \(w_i\), then we calculate the <em>weighted error rate</em> = 1 - weighted accuracy</p>
\[r_j=\frac{\sum_{\hat y^{(j)}_i\neq y_i}w_i}{\sum_{i=1}^Nw_i}
\]
<p>We then calculate the wieght assigned to the \(j\)-th weak learner</p>
\[\alpha_j=\eta\log\left(\frac{1-r_j}{r_j}\right)
\]
<p>\(\eta\) is the learning rate. Finally we update the traning sample weights for \(j+1\)-th weak learner</p>
\[w_i\leftarrow\begin{cases}
w_i, \hat y^{(j)}_i=y_i\\
w_i\exp(\alpha_j), \hat y^{(j)}_i\neq y_i
\end{cases}
\]
<h4><a id="gradient-boosting" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Gradient boosting</h4>
<p><em>Gradient boosting</em> is iteratively building an ensenble of weak learners where a learner is directly trained to model the previous learner's errors. Suppose we have built the first \(j\) weak learners, we build the \(j+1\)-th weak learner by trained to learn to predict ther residual \(r_j\) of the previous learner, and set \(h_{j+1}(X)=\hat r_j\) as its estimate of the residual, and then calculate the residual of this weak learner \(r_{j+1}=r_j-h_j(X)\). By the end the strong learner \(h(X)\) found is the sum of all the weak learners \(h_j(X)\).</p>
<p><em>XGBoost (extreme Gradient boosting)</em> is a specific implementation of gradient boosting that is optimized for performance, efficiency, and scalability. So it is very popular.</p>
<h3><a id="time-series" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Time series</h3>
<ol>
<li>A <em>time series</em> is a sequence of data points \(\{(\mathbf x_t,y_t)\}\) where \(\mathbf x_t\) is a collection of features, \(y_t\) is a numeric variable of interest, and \(t\) stands for time.</li>
<li>Given a time series \(\{(\mathbf x_{t_i},y_{t_i})\}_{i=1}^n\), a <em>forecast</em> is $y_t=f(\mathbf x_t,t|{y_\tau}_{\tau&lt;t})+\epsilon_t$.</li>
<li>A model for time series is a series of random variables \(\{y_t\}_{t\in T}\), where \(y_t\) only depends on \(\mathbf x_t,t\), and \(\mathbf x_t\) is a collection of features that only depends on \(t\).</li>
</ol>
<h4><a id="baseline-forecasting-models" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Baseline forecasting models</h4>
<ol>
<li>without trend nor seasonality
<ul>
<li><em>Average forecast</em> assumes \(y_t\) are independent and identically distributed. The forecast \(y_t=\dfrac{1}{n}\sum\limits_{i=1}^ny_i+\epsilon\) takes the historical average.</li>
<li><em>Naive forecast</em> assumes \(y_t\) is a random walk. The forecast \(y_{t}=y_n+\epsilon\) only uses the last observation.</li>
</ul>
</li>
<li>with trend but not seasonality
<ul>
<li>Linear trend forecast assumes \(E(y_t)=\beta t\). The forecast is \(y_t=\hat\beta t+\epsilon\) with \(\hat\beta\) being the average of first differences \(y_{i+1}-y_i\). An intercept term can be added.</li>
<li>Random walk with drift assumes \(y_{t+1}=y_t+\beta+\epsilon\). The forecast is \(y_t=y_n+\hat\beta(t-n)+\epsilon\) with \(\hat\beta\) being the average of first differences.</li>
</ul>
</li>
<li>with seasonality but not trend
<ul>
<li>Seasonal average forecast assume \(\{y_{r+km}\}_{k}\) are independent and identically distributed for each \(0\leq r&lt;m\). The forecast is</li>
</ul>
\[y_t=\dfrac{1}{\lfloor n/m\rfloor+1}\sum\limits_{k=0}^{\lfloor n/m\rfloor}y_{r+km},\quad r=t\mod m
\]
<ul>
<li>Seasonal naive forecast assumes \(\{y_{r+km}\}_{k}\) are random walks. The forecast is</li>
</ul>
\[y_t=y_\tau+\epsilon,\quad \tau=t-\left(\left\lfloor\frac{t-n}{m}\right\rfloor+1\right)m
\]
</li>
</ol>
<h4><a id="stationary-series" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stationary series</h4>
<ol>
<li>A time series is <em>strictly stationary</em> if \(y_{t_1},\cdots,y_{t_n}\) and \(y_{t_1+\tau},\cdots,y_{t_n+\tau}\) has the same joint probability distribution for any \(n,\tau,t_1,\cdots, t_n\). In particular, we would have
<ul>
<li>\(E(y_t)=\mu\) and \(\operatorname{Var}(y_t)=\sigma^2\).</li>
<li>The joint distribution of \(y_{t_1},\cdots,y_{t_n}\) only depends on \(t_{i+1}-t_i\), these are referred to as the <em>lags</em>.</li>
</ul>
</li>
<li>A time series is <em>stationary</em> if
\[E(y_t)=\mu,\qquad\operatorname{Cov}(y_t,y_{t+\tau})=\gamma(\tau)
\]
here \(\gamma(\tau)\) is called the <em>autovariance</em>, and note that \(\operatorname{Var}(y_t)=\gamma(0)=\sigma^2\).</li>
</ol>
<h5><a id="examples" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Examples</h5>
<ol>
<li><em>White noise</em> is a stationary time series with zero mean constant variance and zero correlation between different times.</li>
<li>The first differences \(y_{t+1}-y_t\) of a random walk \(y_{t+1}=y_t+\epsilon\).</li>
<li>A moving average process \(y_t=\beta_0\epsilon_t+\beta_1\epsilon_{t-1}+\cdots+\beta_q\epsilon_{t-q}\).</li>
</ol>
<h5><a id="differencing" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Differencing</h5>
<p>The \(d\)-th differences \(\nabla^{d}y_t=\nabla^{d-1}y_t-\nabla^{d-1}y_{t-1}\) often produce a stationary series from a non-stationary one.</p>
<h4><a id="arima" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>ARIMA</h4>
<ol>
<li>A time series is <em>autoregressive</em> (AR) of order \(p\) if
\[y_t=\alpha_1y_{t-1}+\cdots+\alpha_py_{t-p}+\epsilon_t
\]
</li>
<li>A time series is autoregressive of order \(p\) with moving average noise (ARMA) of order \(q\) if
\[y_t=\alpha_1y_{t-1}+\cdots+\alpha_py_{t-p}+\beta_0\epsilon_t+\beta_1\epsilon_{t-1}+\cdots+\beta_q\epsilon_{t-q}
\]
</li>
<li>An autoregressive integrated moving average model (ARIMA(\(p,d,q\))) is a time series that its \(d\)-th difference is an ARMA(\(p,q\)).</li>
</ol>
</div>
<div id="section3" class="section">
<h2><a id="unsupervised-learning" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Unsupervised Learning</h2>
<h3><a id="principal-components-analysis-following-scikit-learn" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Principal components analysis (Following scikit-learn)</h3>
<p><em>Principal components analysis (PCA)</em> is a <em>dimension reduction</em> algorithm. Its goal is to project into a lower dimensional space that maximizes variance.</p>
<p>Suppose there are \(N\) observations</p>
\[\{\mathbf x^{(i)}=(x^{(i)}_1,\cdots,x^{(i)}_p)\}_{i=1}^N
\]
<p>of \(p\) features \(\mathbf X=(X_1,\cdots,X_p)\), then</p>
\[\mathbb EX_q=\frac{1}{N}\sum_{i=1}^Nx^{(i)}_q, \quad \text{Cov}(X_q,X_r)=\mathbb E[(X_q-\mathbb EX_q)(X_r-\mathbb EX_r)]
\]
<p>Denote \(A=\begin{bmatrix}\mathbf x^{(1)}\\\vdots\\\mathbf x^{(N)}\end{bmatrix}\), and \(\bar A\) whose \(q\)-th column consists of only \(\mathbb EX_q\), then the covariance matrix is</p>
\[\Sigma=\text{Cov}(\mathbf X,\mathbf X)=\mathbb E[(\mathbf X-\mathbb E\mathbf X)^T(\mathbf X-\mathbb E\mathbf X)]=\frac{1}{N-1}(A-\bar A)^T(A-\bar A)
\]
<p>A heuristic algorithm could be</p>
<ol>
<li>Center the dataset so that each feature has zero mean \(\iff A\leftarrow A-\bar A\)</li>
<li>Induction on \(k\). Choose the \(k\)-th weight vector \(\mathbf w^{(k)}=(w^{(k)}_1,\cdots, w^{(k)}_N)^T\in\mathbb R^N\) such that<br />
\(\|\mathbf w^{(k)}\|=1, \qquad \mathbf w^{(k)}\perp\mathbf w^{(i)},\quad\forall i&lt;k\)<br />
that maximizes variance<br />
\(\text{Var}(\mathbf X^T\mathbf w^{(k)})=(\mathbf w^{(k)})^T\text{Var}(\mathbf X)\mathbf w^{(k)}=(\mathbf w^{(k)})^T\Sigma\mathbf w^{(k)}\)</li>
</ol>
<p>This is just <em>singular value decomposition</em> for \(A-\bar A\). Suppose<br />
\(A-\bar A=V^TSW\)<br />
is the singular decomposition, then<br />
\(\Sigma=W^T\frac{S^2}{N-1}W\)</p>
<p>The \(k\)-th principal component of \(\mathbf x^{(i)}\) is \(\mathbf x^{(i)}\cdot\mathbf w^{(k)}\). The <em>explained variances</em> are the diagonal elements in \(\dfrac{S^2}{N-1}\).</p>
<h3><a id="t-distributed-stochastic-neighbor-embedding" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>\(t\)-distributed stochastic neighbor embedding</h3>
<p><em>\(t\)-distributed stochastic neighbor embedding (tSNE)</em> typically reduce the dimension of the set of \(m\) features down to 2 to 3 for visualization. Suppose \(y_i\) is a low dimensional projection of \(x_i\), we could define conditional probabilities</p>
\[p_{j|i}=\frac{\exp(-\|x_i-x_j\|^2/2\sigma_i^2)}{\sum_{k\neq i}\exp(-\|x_i-x_k\|^2/2\sigma_i^2)}
\]
\[q_{j|i}=\frac{(1+\|y_i-y_j\|^2)^{-1}}{\sum_{k\neq i}(1+\|y_i-y_k\|^2)^{-1}}
\]
<p>Assuming $p_{i|i}=q_{i|i}=0$. $p_{j|i}$ and $q_{j|i}$ are expected to be close. We can choose the cost function to be KL convergence, this would optimize \(y_i\)'s.</p>
<h4><a id="pros-cons" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Pros &amp; Cons</h4>
<ol>
<li>Since it is stochastic, it generates slightly different results each time.</li>
<li>Unlike, this is not reusable on making predictions for new data.</li>
<li>The magnitude of the distances between clusters shouldn't be interpreted.</li>
<li>tSNE results should not be used as statistical evidence or proof of something, and it sometimes can produce clusters that aren't actually true. Thus it is always a good practice to run it a few times to ensure that the cluster persists.</li>
</ol>
<h3><a id="k-means-clustering" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>\(K\) means clustering</h3>
<p><em>\(K\) means clustering</em> tries to divide a dataset into \(k\) clusters. Start with random guess of \(k\) centroids. Then group all points according to distance to the centroids. Recalculate the centroid as the average of each group. Repeat these steps until you see no change of groups.</p>
<h4><a id="how-to-choose-the-best-k" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>How to choose the best \(K\)?</h4>
<p>Typically we run the algorithm multiple times examing the behaviour of the model depending on different values of \(K\) according to some metric, and then choose the best.</p>
<ol>
<li>The <em>elbow method</em>. We first calculate the <em>inertia</em> of the resulting clustering, which is defined to be
\[\sum_{i=1}^n\operatorname{dist}(X^{(i)},c^{(i)})^2
\]
And need to find the &quot;elbow&quot; in the graph.</li>
<li>The <em>Silhouette method</em>. The <em>Silhouette score</em> for the data point \(x_i\) with \(i\) in cluster \(I\) is defined to be
\[\dfrac{b-a}{\max(a,b)}=\begin{cases}
1-a/b, a&lt;b\\
0, a=b\\
b/a-1, a&gt;b
\end{cases}
\]
Where $a=\dfrac{1}{|I|-1}\sum_{i\in I,i\neq j}d(x_i,d_j)$ is the average of the distances between \(x_i\) and other points with indices in \(I\) and $b=\min\limits_{J\neq I}\dfrac{1}{|J|}\sum_{j\in J}d(x_i,x_j)$ is the minimal average of distances between \(x_i\) with all points with indices in some \(J\neq I\). Note that this score ranges from -1 to 1. The higher the score, the better the clustering.</li>
</ol>
<p>We can use it to generate <em>silhouette plots</em>.</p>
<h3><a id="hierarchical-clustering" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Hierarchical clustering</h3>
<p><em>Hierarchical clustering</em> starts with each point as its own cluster and work its way up by merging clusters, generating a <em>dendrogram</em>, to have a measure for deciding when to merge clusters, we need cluster <em>linkage</em></p>
<ol>
<li><em>single linkage</em>. The minimal distance between two points in two clusters.</li>
<li><em>complete linkage</em>. The maximal distance between two points in two clusters.</li>
<li><em>centroid linkage</em>. The distance between centroids.</li>
</ol>
</div>
<div id="section4" class="section">
<h2><a id="neural-networks" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Neural Networks</h2>
<p>Start with \(n\) observations with \(m\) features</p>
<h4><a id="perceptron" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Perceptron</h4>
<p>A <em>perceptron</em> is to neutron is as a artificial neural network is to an actual neural network. With a predefined <em>activation function</em> (some non linear function) It output</p>
\[\hat y=\sigma(w_1x_1+\cdots+w_mx_m+b)=\sigma(\mathbf x\cdot\mathbf w)
\]
<p>here augmented \(\mathbf x\) by 1 and \(\mathbf w\) by \(b\) adds a <em>bias</em> term. The decision boudary is still linear which is not ideal, so we need to introduce multilayer neural network.</p>
<h4><a id="feed-forward-network-architecture" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Feed forward network architecture</h4>
<p>A <em>feed forward network architecture</em> is a multilayered neural network where each layer consists of many perceptrons. Feeding forward each layer is equivalent to a matrix multiplication. So terms of equations</p>
\[h_1=\sigma(W_1\mathbf x),\cdots,\hat y=\sigma(W_{k+1}\mathbf x_k)
\]
<h4><a id="backpropagation" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Backpropagation</h4>
<p>To adjust the weights in the neural network, we need <em>back propagation</em>. If we take the loss function to be \(\ell=(\hat y-y)^2\), we know that \(\nabla\ell\) can be computed using the chain rule. Then we need to update weights by \(\mathbf w\leftarrow\mathbf w-\eta\nabla\ell(\mathbf w)\). This process should run through the whole of training points. A complete cycle is referred to as an <em>epoch</em>.</p>
<h4><a id="convolution-neural-network" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Convolution neural network</h4>
<p><em>Convolution layers</em> perform convolutions over the image (square matrix of data points). A <em>pooling layer</em> with a stride takes the maximal/minimal/average of points in that layer, which could downsample our observations or degrade the image. Then we feed into a fully connected layer. The reason for pooling is that the computation in the dense layer could be huge.</p>
<p><em>Padding</em> is simply add zeros or constants at the boundary of the image.</p>
<h4><a id="recurrent-neural-network" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Recurrent neural network</h4>
<p>The set up in equations is</p>
\[h_1=\sigma(W_{xh}X^{(1)}),\cdots,h_t=\sigma(W_{xh}X^{(t)}+W_{hh}h_{t-1}),\hat y^{(t)}=\sigma'(W_{hy}h_t)
\]
<h3><a id="long-short-term-memory" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Long short-term memory</h3>
<p><em>Long short-term memory (LSTM)</em> improve RNN model that overcomes the the issue of vanishing gradient and capture the long term dependencies much better than RNN.</p>
<h3><a id="transformer-model" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Transformer model</h3>
<ol>
<li>Input embedding</li>
<li>Positional embedding</li>
<li>Multi-head Attention</li>
</ol>
</div>
</div>












<script>
document.querySelectorAll('.sidebar a').forEach(link => {
    link.addEventListener('click', function(e) {
        e.preventDefault();
        // Remove active class from all links
        document.querySelectorAll('.sidebar a').forEach(item => {
            item.classList.remove('active');
        });
        // Add active class to the clicked link
        this.classList.add('active');
        // Hide all sections
        document.querySelectorAll('.section').forEach(section => {
            section.style.display = 'none';
        });
        // Show the clicked section
        const targetSection = document.querySelector(this.getAttribute('href'));
        if (targetSection) {
            targetSection.style.display = 'block';
        }
    });
});
// Display the first section by default
document.querySelector('#section0').style.display = 'block';
</script>
</body>

</html>